{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from project_utils.autosave_plots import enable_autosave\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from sklearn.tree import export_text\n",
    "from tqdm.auto import tqdm\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save plots to results/\n",
    "enable_autosave(\"ml_baselines\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load the cleaned up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/creditcard_clean.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Creating the training/validation/testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out class from other features\n",
    "X = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n",
    "# split the data 80:20 into train and test_validation (we'll split the latter again)\n",
    "X_train, X_test_validation, y_train, y_test_validation = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "# split the test_validation data into test and validation\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(\n",
    "    X_test_validation,\n",
    "    y_test_validation,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "for name, labels in zip(\n",
    "    [\"Train\", \"Validation\", \"Test\"], [y_train, y_validation, y_test]\n",
    "):\n",
    "    print(f\"{name}: {len(labels)} samples, {labels.mean()*100:.3f}% fraud\")\n",
    "\n",
    "print(\n",
    "    f\"Shapes: X_train={X_train.shape}, X_validation={X_validation.shape}, X_test={X_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out class from other features\n",
    "X = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n",
    "# split the data 80:20 into train and test_validation (we'll split the latter again)\n",
    "X_train, X_test_validation, y_train, y_test_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# split the test_validation data into test and validation\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(\n",
    "    X_test_validation,\n",
    "    y_test_validation,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=y_test_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "for name, labels in zip(\n",
    "    [\"Train\", \"Validation\", \"Test\"], [y_train, y_validation, y_test]\n",
    "):\n",
    "    print(f\"{name}: {len(labels)} samples, {labels.mean()*100:.3f}% fraud\")\n",
    "\n",
    "print(\n",
    "    f\"Shapes: X_train={X_train.shape}, X_validation={X_validation.shape}, X_test={X_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define logistic regression\n",
    "log_reg = LogisticRegression(\n",
    "    class_weight=\"balanced\",  # dataset is heavily imbalanced so classes are weighted\n",
    "    penalty=\"l2\",\n",
    "    C=1,\n",
    "    solver=\"lbfgs\",\n",
    "    random_state=SEED,\n",
    "    max_iter=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation_probabilities = log_reg.predict_proba(X_validation)\n",
    "y_validation_probabilities = y_validation_probabilities[\n",
    "    :, 1\n",
    "]  # probabilities of fraud class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_lr = average_precision_score(y_validation, y_validation_probabilities)\n",
    "print(f\"Average APS score: {aps_lr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_lr, recall_lr, _ = precision_recall_curve(\n",
    "    y_validation, y_validation_probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps_v14 = average_precision_score(\n",
    "    y_validation, -X_validation[\"V14\"]\n",
    ")  # flipped sign because V14 mean is greater than sample mean\n",
    "precision_v14, recall_v14, _ = precision_recall_curve(\n",
    "    y_validation, -X_validation[\"V14\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# plot LT PRC\n",
    "plt.plot(precision_lr, recall_lr, lw=2, label=f\"LT | APS = {aps_lr:.3f}\")\n",
    "plt.fill_between(precision_lr, recall_lr, alpha=0.2)\n",
    "\n",
    "# plot V14 VRC\n",
    "plt.plot(precision_v14, recall_v14, lw=2, label=f\"V14 | APS = {aps_v14:.3f}\")\n",
    "plt.fill_between(precision_v14, recall_v14, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Recall (frauds caught)\")\n",
    "plt.ylabel(\"Precision (alerts correct)\")\n",
    "plt.title(\"Precision-Recall Curve | V14 vs Logistic Regression\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,  # use all processors\n",
    "    random_state=SEED,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# train\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Visualize a sample tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize tree\n",
    "print(\"Single Decision Tree\")\n",
    "print(export_text(rf.estimators_[0], feature_names=list(X_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize tree probabilities\n",
    "print(\"Single Decision Tree Weighted Class Count\")\n",
    "print(rf.estimators_[0].tree_.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual implementation\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,  # use all processors\n",
    "    random_state=SEED,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# train\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "y_validation_probabilities = rf.predict_proba(X_validation)\n",
    "y_validation_probabilities = y_validation_probabilities[\n",
    "    :, 1\n",
    "]  # probabilities of fraud class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate aps\n",
    "aps_rf = average_precision_score(y_validation, y_validation_probabilities)\n",
    "print(f\"Average APS score: {aps_rf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_rf, recall_rf, _ = precision_recall_curve(\n",
    "    y_validation, y_validation_probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# plot V14 VRC\n",
    "plt.plot(precision_v14, recall_v14, lw=2, label=f\"V14 | APS = {aps_v14:.3f}\")\n",
    "plt.fill_between(precision_v14, recall_v14, alpha=0.2)\n",
    "\n",
    "# plot LT PRC\n",
    "plt.plot(precision_lr, recall_lr, lw=2, label=f\"LT | APS = {aps_lr:.3f}\")\n",
    "plt.fill_between(precision_lr, recall_lr, alpha=0.2)\n",
    "\n",
    "# plot RF PRC\n",
    "plt.plot(precision_rf, recall_rf, lw=2, label=f\"RF | APS = {aps_rf:.3f}\")\n",
    "plt.fill_between(precision_rf, recall_rf, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Recall (frauds caught)\")\n",
    "plt.ylabel(\"Precision (alerts correct)\")\n",
    "plt.title(\"Precision-Recall Curve | V14 vs Logistic Regression vs Random Forest\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial model training\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=2000,  # number of trees\n",
    "    max_depth=6,\n",
    "    learning_rate=0.3,  # eta\n",
    "    verbosity=0,\n",
    "    gamma=0,\n",
    "    min_child_weight=1,\n",
    "    max_delta_step=0,\n",
    "    subsample=0.8,  # subsample 80% of instances to reduce overfitting\n",
    "    sampling_method=\"uniform\",\n",
    "    colsample_bytree=0.8,  # subsample 80% of features to reduce dependence on few features\n",
    "    reg_alpha=0,  # L1 regularization\n",
    "    reg_lambda=1,  # L2 regularization\n",
    "    scale_pos_weight=(y_train == 0).sum()\n",
    "    / (y_train == 1).sum(),  # balance imbalance in dataset\n",
    "    random_state=SEED,\n",
    "    eval_metric=\"aucpr\",\n",
    ")\n",
    "\n",
    "# train\n",
    "xgb.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_validation, y_validation)],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "y_validation_probabilities = xgb.predict_proba(X_validation)\n",
    "y_validation_probabilities = y_validation_probabilities[\n",
    "    :, 1\n",
    "]  # probabilities of fraud class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate aps\n",
    "aps_xgb = average_precision_score(y_validation, y_validation_probabilities)\n",
    "print(f\"Average APS score: {aps_xgb:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Mini training sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ParamSpace = Dict[str, List[float]]\n",
    "\n",
    "hyperparameter_space: ParamSpace = {\n",
    "    \"learning_rate\": [0.03, 0.05, 0.1],\n",
    "    \"max_depth\": [1, 3, 4, 5, 10],\n",
    "    \"min_child_weight\": [1, 2, 4, 10],\n",
    "}\n",
    "\n",
    "results = []\n",
    "for lr in tqdm(\n",
    "    hyperparameter_space[\"learning_rate\"], desc=\"lr\", position=0, leave=True\n",
    "):\n",
    "    for md in tqdm(\n",
    "        hyperparameter_space[\"max_depth\"], desc=\"md\", position=1, leave=False\n",
    "    ):\n",
    "        for mcw in tqdm(\n",
    "            hyperparameter_space[\"min_child_weight\"],\n",
    "            desc=\"mcw\",\n",
    "            position=2,\n",
    "            leave=False,\n",
    "        ):\n",
    "            model = XGBClassifier(\n",
    "                n_estimators=2000,  # number of trees\n",
    "                max_depth=md,\n",
    "                learning_rate=lr,  # eta\n",
    "                verbosity=0,\n",
    "                n_jobs=-1,\n",
    "                gamma=0,\n",
    "                min_child_weight=mcw,\n",
    "                max_delta_step=0,\n",
    "                subsample=0.8,  # subsample 80% of rows to reduce overfitting\n",
    "                sampling_method=\"uniform\",\n",
    "                colsample_bytree=0.8,  # subsample 80% of features to reduce dependence on few features\n",
    "                reg_alpha=0,  # L1 regularization\n",
    "                reg_lambda=1,  # L2 regularization\n",
    "                scale_pos_weight=(y_train == 0).sum()\n",
    "                / (y_train == 1).sum(),  # balance imbalance in dataset\n",
    "                random_state=SEED,\n",
    "                eval_metric=\"aucpr\",\n",
    "            )\n",
    "            model.fit(\n",
    "                X_train, y_train, eval_set=[(X_validation, y_validation)], verbose=False\n",
    "            )\n",
    "\n",
    "            # predict APS\n",
    "            y_validation_probabilities = model.predict_proba(X_validation)\n",
    "            y_validation_probabilities = y_validation_probabilities[\n",
    "                :, 1\n",
    "            ]  # probabilities of fraud class\n",
    "            aps = average_precision_score(y_validation, y_validation_probabilities)\n",
    "            results.append(\n",
    "                {\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"max_depth\": md,\n",
    "                    \"min_child_weight\": mcw,\n",
    "                    \"val_aps\": aps,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_df = (\n",
    "    pd.DataFrame(results).sort_values(\"val_aps\", ascending=False).reset_index(drop=True)\n",
    ")\n",
    "print(tune_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the best model with early stopping\n",
    "## initial model training\n",
    "early_stopping_rounds_results = []\n",
    "for early_stopping_rounds in tqdm(range(10, 200, 10)):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=2000,  # number of trees\n",
    "        max_depth=tune_df[\"max_depth\"][0],\n",
    "        learning_rate=tune_df[\"learning_rate\"][0],  # eta\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        gamma=0,\n",
    "        min_child_weight=tune_df[\"min_child_weight\"][0],\n",
    "        max_delta_step=0,\n",
    "        subsample=0.8,  # subsample 80% of rows to reduce overfitting\n",
    "        sampling_method=\"uniform\",\n",
    "        colsample_bytree=0.8,  # subsample 80% of features to reduce dependence on few features\n",
    "        reg_alpha=0,  # L1 regularization\n",
    "        reg_lambda=1,  # L2 regularization\n",
    "        scale_pos_weight=(y_train == 0).sum()\n",
    "        / (y_train == 1).sum(),  # balance imbalance in dataset\n",
    "        random_state=SEED,\n",
    "        eval_metric=\"aucpr\",\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_validation, y_validation)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # predict APS\n",
    "    y_validation_probabilities = model.predict_proba(X_validation)\n",
    "    y_validation_probabilities = y_validation_probabilities[\n",
    "        :, 1\n",
    "    ]  # probabilities of fraud class\n",
    "    aps = average_precision_score(y_validation, y_validation_probabilities)\n",
    "    early_stopping_rounds_results.append(\n",
    "        {\n",
    "            \"early_stopping_rounds\": early_stopping_rounds,\n",
    "            \"best_iteration\": model.best_iteration,\n",
    "            \"num_boosted_rounds\": model.get_booster().num_boosted_rounds(),\n",
    "            \"val_aps\": aps,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Early stopping rounds by APS\")\n",
    "print(pd.DataFrame(early_stopping_rounds_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_early_stopping_rounds = (\n",
    "    pd.DataFrame(early_stopping_rounds_results)\n",
    "    .sort_values(by=[\"val_aps\", \"early_stopping_rounds\"], ascending=[False, True])\n",
    "    .reset_index(drop=True)[\"early_stopping_rounds\"][0]\n",
    ")\n",
    "\n",
    "print(f\"Optimal early stopping rounds: {optimal_early_stopping_rounds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "#### Broader grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_space_regularization: ParamSpace = {\n",
    "    \"reg_alpha\": [0.0, 0.1, 0.5],\n",
    "    \"reg_lambda\": [0.5, 1.0, 2.0, 5.0],\n",
    "    \"gamma\": [0.0, 0.1, 1.0],\n",
    "}\n",
    "hyperparameters_list: List[Dict[str, float]] = list(\n",
    "    ParameterGrid(hyperparameter_space_regularization)\n",
    ")\n",
    "\n",
    "broad_grid_search_results: List[Dict[str, float]] = []\n",
    "for hyperparameters in tqdm(hyperparameters_list):\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=2000,  # number of trees\n",
    "        max_depth=tune_df[\"max_depth\"][0],\n",
    "        learning_rate=tune_df[\"learning_rate\"][0],  # eta\n",
    "        verbosity=0,\n",
    "        n_jobs=-1,\n",
    "        min_child_weight=tune_df[\"min_child_weight\"][0],\n",
    "        max_delta_step=0,\n",
    "        subsample=0.8,  # subsample 80% of instances to reduce overfitting\n",
    "        sampling_method=\"uniform\",\n",
    "        colsample_bytree=0.8,  # subsample 80% of features to reduce dependence on few features\n",
    "        scale_pos_weight=(y_train == 0).sum()\n",
    "        / (y_train == 1).sum(),  # balance imbalance in dataset\n",
    "        random_state=SEED,\n",
    "        eval_metric=\"aucpr\",\n",
    "        early_stopping_rounds=optimal_early_stopping_rounds,\n",
    "        **hyperparameters,\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=[(X_validation, y_validation)], verbose=False)\n",
    "\n",
    "    # predict APS\n",
    "    y_validation_probabilities = model.predict_proba(X_validation)\n",
    "    y_validation_probabilities = y_validation_probabilities[\n",
    "        :, 1\n",
    "    ]  # probabilities of fraud class\n",
    "    aps = average_precision_score(y_validation, y_validation_probabilities)\n",
    "    broad_grid_search_results.append(\n",
    "        {\n",
    "            \"reg_alpha\": hyperparameters[\"reg_alpha\"],\n",
    "            \"reg_lambda\": hyperparameters[\"reg_lambda\"],\n",
    "            \"gamma\": hyperparameters[\"gamma\"],\n",
    "            \"val_aps\": aps,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_parameters = (\n",
    "    pd.DataFrame(broad_grid_search_results)\n",
    "    .sort_values(by=\"val_aps\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(regularization_parameters.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "    n_estimators=2000,  # number of trees\n",
    "    max_depth=tune_df[\"max_depth\"][0],\n",
    "    learning_rate=tune_df[\"learning_rate\"][0],  # eta\n",
    "    verbosity=0,\n",
    "    n_jobs=-1,\n",
    "    gamma=regularization_parameters[\"gamma\"][0],\n",
    "    min_child_weight=tune_df[\"min_child_weight\"][0],\n",
    "    max_delta_step=0,\n",
    "    subsample=0.8,  # subsample 80% of instances to reduce overfitting\n",
    "    sampling_method=\"uniform\",\n",
    "    colsample_bytree=0.8,  # subsample 80% of features to reduce dependence on few features\n",
    "    reg_alpha=regularization_parameters[\"reg_alpha\"][0],  # L1 regularization\n",
    "    reg_lambda=regularization_parameters[\"reg_lambda\"][0],  # L2 regularization\n",
    "    scale_pos_weight=(y_train == 0).sum()\n",
    "    / (y_train == 1).sum(),  # balance imbalance in dataset\n",
    "    random_state=SEED,\n",
    "    eval_metric=\"aucpr\",\n",
    "    early_stopping_rounds=optimal_early_stopping_rounds,\n",
    ")\n",
    "model.fit(X_train, y_train, eval_set=[(X_validation, y_validation)], verbose=False)\n",
    "\n",
    "# predict APS\n",
    "y_validation_probabilities = model.predict_proba(X_validation)\n",
    "y_validation_probabilities = y_validation_probabilities[\n",
    "    :, 1\n",
    "]  # probabilities of fraud class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate aps\n",
    "aps_xgb = average_precision_score(y_validation, y_validation_probabilities)\n",
    "print(f\"Average APS score: {aps_xgb:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision and recall values\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(\n",
    "    y_validation, y_validation_probabilities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# plot V14 VRC\n",
    "plt.plot(precision_v14, recall_v14, lw=2, label=f\"V14 | APS = {aps_v14:.3f}\")\n",
    "plt.fill_between(precision_v14, recall_v14, alpha=0.2)\n",
    "\n",
    "# plot LT PRC\n",
    "plt.plot(precision_lr, recall_lr, lw=2, label=f\"LT | APS = {aps_lr:.3f}\")\n",
    "plt.fill_between(precision_lr, recall_lr, alpha=0.2)\n",
    "\n",
    "# plot RF PRC\n",
    "plt.plot(precision_rf, recall_rf, lw=2, label=f\"RF | APS = {aps_rf:.3f}\")\n",
    "plt.fill_between(precision_rf, recall_rf, alpha=0.2)\n",
    "\n",
    "# plot XGB PRC\n",
    "plt.plot(precision_xgb, recall_xgb, lw=2, label=f\"XGB | APS = {aps_xgb:.3f}\")\n",
    "plt.fill_between(precision_xgb, recall_xgb, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Recall (frauds caught)\")\n",
    "plt.ylabel(\"Precision (alerts correct)\")\n",
    "plt.title(\n",
    "    \"Precision-Recall Curve | V14 vs Logistic Regression vs Random Forest vs XGBoost\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot aps by model\n",
    "model_names = [\"V14\", \"Logistic Regression\", \"Random Forest\", \"XGBoost\"]\n",
    "aps_scores = [aps_v14, aps_lr, aps_rf, aps_xgb]\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(model_names, aps_scores, color=[\"gray\", \"blue\", \"green\", \"orange\"])\n",
    "plt.ylabel(\"Average Precision Score (APS)\")\n",
    "plt.title(\"Model Comparison on Validation Set\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores for V14\n",
    "aps_v14 = average_precision_score(\n",
    "    y_test, -X_test[\"V14\"]\n",
    ")  # flipped sign because V14 mean is greater than sample mean\n",
    "precision_v14, recall_v14, _ = precision_recall_curve(y_test, -X_test[\"V14\"])\n",
    "\n",
    "# scores for LR\n",
    "y_test_probabilities = log_reg.predict_proba(X_test)\n",
    "y_test_probabilities = y_test_probabilities[:, 1]  # probabilities of fraud class\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(y_test, y_test_probabilities)\n",
    "\n",
    "# scores for RF\n",
    "y_test_probabilities = rf.predict_proba(X_test)\n",
    "y_test_probabilities = y_test_probabilities[:, 1]  # probabilities of fraud class\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_test_probabilities)\n",
    "\n",
    "# score for XGB\n",
    "y_test_probabilities = model.predict_proba(X_test)\n",
    "y_test_probabilities = y_test_probabilities[:, 1]  # probabilities of fraud class\n",
    "precision_xgb, recall_xgb, _ = precision_recall_curve(y_test, y_test_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# plot V14 VRC\n",
    "plt.plot(precision_v14, recall_v14, lw=2, label=f\"V14 | APS = {aps_v14:.3f}\")\n",
    "plt.fill_between(precision_v14, recall_v14, alpha=0.2)\n",
    "\n",
    "# plot LT PRC\n",
    "plt.plot(precision_lr, recall_lr, lw=2, label=f\"LT | APS = {aps_lr:.3f}\")\n",
    "plt.fill_between(precision_lr, recall_lr, alpha=0.2)\n",
    "\n",
    "# plot RF PRC\n",
    "plt.plot(precision_rf, recall_rf, lw=2, label=f\"RF | APS = {aps_rf:.3f}\")\n",
    "plt.fill_between(precision_rf, recall_rf, alpha=0.2)\n",
    "\n",
    "# plot XGB PRC\n",
    "plt.plot(precision_xgb, recall_xgb, lw=2, label=f\"XGB | APS = {aps_xgb:.3f}\")\n",
    "plt.fill_between(precision_xgb, recall_xgb, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Recall (frauds caught)\")\n",
    "plt.ylabel(\"Precision (alerts correct)\")\n",
    "plt.title(\"Precision-Recall Curve | Test Set | V14 vs Classic ML Baselines\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (credit-card-fraud-detection)",
   "language": "python",
   "name": "credit-card-fraud-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
