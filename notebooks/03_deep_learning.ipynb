{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_utils.autosave_plots import enable_autosave\n",
    "from project_utils.plotting import plot_loss_curves\n",
    "\n",
    "from typing import TypedDict, Tuple, Callable, Sized, cast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.container import BarContainer\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save plots to results/\n",
    "enable_autosave(\"deep_learning\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set function for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def set_seed(seed: int = SEED) -> None:\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Load the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/creditcard_clean.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Prepare training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out class from other features\n",
    "X = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n",
    "# split the data 80:20 into train and test_validation (we'll split the latter again)\n",
    "X_train, X_test_validation, y_train, y_test_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "# split the test_validation data into test and validation\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(\n",
    "    X_test_validation,\n",
    "    y_test_validation,\n",
    "    test_size=0.5,\n",
    "    random_state=SEED,\n",
    "    stratify=y_test_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution\n",
    "for name, labels in zip(\n",
    "    [\"Train\", \"Validation\", \"Test\"], [y_train, y_validation, y_test]\n",
    "):\n",
    "    print(f\"{name}: {len(labels)} samples, {labels.mean()*100:.3f}% fraud\")\n",
    "\n",
    "print(\n",
    "    f\"Shapes: X_train={X_train.shape}, X_validation={X_validation.shape}, X_test={X_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Tensorification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy arrays\n",
    "X_train_np = X_train.values.astype(np.float32)\n",
    "y_train_np = y_train.values.astype(np.float32)\n",
    "\n",
    "X_validation_np = X_validation.values.astype(np.float32)\n",
    "y_validation_np = y_validation.values.astype(np.float32)\n",
    "\n",
    "X_test_np = X_test.values.astype(np.float32)\n",
    "y_test_np = y_test.values.astype(np.float32)\n",
    "\n",
    "# convert to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train_np)\n",
    "y_train_tensor = torch.from_numpy(y_train_np).unsqueeze(\n",
    "    1\n",
    ")  # add extra dim for BCEWithLogitsLoss\n",
    "\n",
    "X_validation_tensor = torch.from_numpy(X_validation_np)\n",
    "y_validation_tensor = torch.from_numpy(y_validation_np).unsqueeze(\n",
    "    1\n",
    ")  # add extra dim for BCEWithLogitsLoss\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test_np)\n",
    "y_test_tensor = torch.from_numpy(y_test_np).unsqueeze(\n",
    "    1\n",
    ")  # add extra dim for BCEWithLogitsLoss\n",
    "\n",
    "# create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "validation_dataset = TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# create DataLoaders\n",
    "Batch = Tuple[torch.Tensor, torch.Tensor]\n",
    "set_seed(SEED)\n",
    "train_loader: DataLoader[Batch] = DataLoader(\n",
    "    cast(Dataset[Batch], train_dataset), batch_size=32, shuffle=True\n",
    ")\n",
    "set_seed(SEED)\n",
    "validation_loader: DataLoader[Batch] = DataLoader(\n",
    "    cast(Dataset[Batch], validation_dataset), batch_size=32, shuffle=False\n",
    ")\n",
    "set_seed(SEED)\n",
    "test_loader: DataLoader[Batch] = DataLoader(\n",
    "    cast(Dataset[Batch], test_dataset), batch_size=32, shuffle=False\n",
    ")\n",
    "\n",
    "# define class imbalance weights\n",
    "pos_weight_value = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32).to(\"cpu\")\n",
    "print(f\"Positive class weight: {pos_weight.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Define training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define type for criterion\n",
    "Criterion = Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n",
    "\n",
    "\n",
    "class TrainResults(TypedDict):\n",
    "    name: str\n",
    "    model: nn.Module\n",
    "    train_losses: list[float]\n",
    "    val_losses: list[float]\n",
    "    val_aps: float\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    name: str,\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: Criterion,\n",
    "    train_loader: DataLoader[Batch],\n",
    "    X_val_tensor: torch.Tensor,\n",
    "    y_val_np: np.ndarray,\n",
    "    num_epochs: int = 30,\n",
    ") -> TrainResults:\n",
    "    train_losses: list[float] = []\n",
    "    val_losses: list[float] = []\n",
    "    dataset_size = len(cast(Sized, train_loader.dataset))\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=f\"Training: {name}\"):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)  # (batch, 1)\n",
    "            loss = criterion(outputs, y_batch)  # BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        epoch_train_loss = running_loss / dataset_size\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_val_tensor)\n",
    "            val_loss = criterion(\n",
    "                val_logits, torch.from_numpy(y_val_np).unsqueeze(1).float()\n",
    "            )\n",
    "            epoch_val_loss = val_loss.item()\n",
    "            val_losses.append(epoch_val_loss)\n",
    "\n",
    "    # final APS\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(X_val_tensor)\n",
    "        val_probs = torch.sigmoid(val_logits).numpy().ravel()\n",
    "    aps = average_precision_score(y_val_np, val_probs)\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"model\": model,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_aps\": aps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect experiments and models\n",
    "experiments = {}\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditCardFraudModel(nn.Module):\n",
    "    def __init__(self, input_dim: int) -> None:\n",
    "        super(CreditCardFraudModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return cast(torch.Tensor, self.network(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### With class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v0 with pos_weights\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v0 = CreditCardFraudModel(input_dim=X_train.shape[1])\n",
    "criterion_v0 = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer_v0 = torch.optim.Adam(model_v0.parameters(), lr=0.001)\n",
    "\n",
    "result_v0 = train_and_evaluate_model(\n",
    "    name=\"MLP_v0_pos_weights\",\n",
    "    model=model_v0,\n",
    "    criterion=criterion_v0,\n",
    "    optimizer=optimizer_v0,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v0[\"name\"]] = {\n",
    "    \"version\": 0,\n",
    "    \"val_aps\": result_v0[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v0[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v0[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v0[\"name\"]] = result_v0[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v0['name']}\")\n",
    "print(f\"Validation APS: {result_v0['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v0[\"train_losses\"],\n",
    "    result_v0[\"val_losses\"],\n",
    "    title=f\"{result_v0['name']} Loss Curves\",\n",
    "    smoothing=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Without class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v0.1 without pos_weights\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v0_1 = CreditCardFraudModel(input_dim=X_train.shape[1])\n",
    "criterion_v0_1 = nn.BCEWithLogitsLoss(pos_weight=None)\n",
    "optimizer_v0_1 = torch.optim.Adam(model_v0_1.parameters(), lr=0.001)\n",
    "\n",
    "result_v0_1 = train_and_evaluate_model(\n",
    "    name=\"MLP_v0.1_without_pos_weights\",\n",
    "    model=model_v0_1,\n",
    "    criterion=criterion_v0_1,\n",
    "    optimizer=optimizer_v0_1,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v0_1[\"name\"]] = {\n",
    "    \"version\": 0.1,\n",
    "    \"val_aps\": result_v0_1[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v0_1[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v0_1[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v0_1[\"name\"]] = result_v0_1[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v0_1['name']}\")\n",
    "print(f\"Validation APS: {result_v0_1['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v0_1[\"train_losses\"],\n",
    "    result_v0_1[\"val_losses\"],\n",
    "    title=f\"{result_v0_1['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditCardFraudModelDense(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, use_dropout: bool = False, p: float = 0.3\n",
    "    ) -> None:\n",
    "        super(CreditCardFraudModelDense, self).__init__()\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(p=p))\n",
    "\n",
    "        layers.append(nn.Linear(32, 1))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return cast(torch.Tensor, self.network(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v1 without pos_weights\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v1 = CreditCardFraudModelDense(input_dim=X_train.shape[1])\n",
    "criterion_v1 = nn.BCEWithLogitsLoss(pos_weight=None)\n",
    "optimizer_v1 = torch.optim.Adam(model_v1.parameters(), lr=0.001)\n",
    "\n",
    "result_v1 = train_and_evaluate_model(\n",
    "    name=\"MLP_v1_without_pos_weights\",\n",
    "    model=model_v1,\n",
    "    criterion=criterion_v1,\n",
    "    optimizer=optimizer_v1,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v1[\"name\"]] = {\n",
    "    \"version\": 1,\n",
    "    \"val_aps\": result_v1[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v1[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v1[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v1[\"name\"]] = result_v1[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v1['name']}\")\n",
    "print(f\"Validation APS: {result_v1['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v1[\"train_losses\"],\n",
    "    result_v1[\"val_losses\"],\n",
    "    title=f\"{result_v1['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v1.1 without pos_weights 1e-4 lr\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v1_1 = CreditCardFraudModelDense(input_dim=X_train.shape[1])\n",
    "criterion_v1_1 = nn.BCEWithLogitsLoss(pos_weight=None)\n",
    "optimizer_v1_1 = torch.optim.Adam(model_v1_1.parameters(), lr=0.0001)\n",
    "\n",
    "result_v1_1 = train_and_evaluate_model(\n",
    "    name=\"MLP_v1.1_without_pos_weights\",\n",
    "    model=model_v1_1,\n",
    "    criterion=criterion_v1_1,\n",
    "    optimizer=optimizer_v1_1,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v1_1[\"name\"]] = {\n",
    "    \"version\": 1.1,\n",
    "    \"val_aps\": result_v1_1[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v1_1[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v1_1[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v1_1[\"name\"]] = result_v1_1[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v1_1['name']}\")\n",
    "print(f\"Validation APS: {result_v1_1['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v1_1[\"train_losses\"],\n",
    "    result_v1_1[\"val_losses\"],\n",
    "    title=f\"{result_v1_1['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 2 layers + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v1.2 without pos_weights,  1e-4 lr, dropout 0.3\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v1_2 = CreditCardFraudModelDense(\n",
    "    input_dim=X_train.shape[1], use_dropout=True, p=0.3\n",
    ")\n",
    "criterion_v1_2 = nn.BCEWithLogitsLoss(pos_weight=None)\n",
    "optimizer_v1_2 = torch.optim.Adam(model_v1_2.parameters(), lr=0.0001)\n",
    "\n",
    "result_v1_2 = train_and_evaluate_model(\n",
    "    name=\"MLP_v1.2_without_pos_weights\",\n",
    "    model=model_v1_2,\n",
    "    criterion=criterion_v1_2,\n",
    "    optimizer=optimizer_v1_2,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v1_2[\"name\"]] = {\n",
    "    \"version\": 1.2,\n",
    "    \"val_aps\": result_v1_2[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v1_2[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v1_2[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v1_2[\"name\"]] = result_v1_2[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v1_2['name']}\")\n",
    "print(f\"Validation APS: {result_v1_2['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v1_2[\"train_losses\"],\n",
    "    result_v1_2[\"val_losses\"],\n",
    "    title=f\"{result_v1_2['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 2 layers + dropout + mild class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v1.3 without pos_weights,  1e-4 lr, dropout 0.3\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v1_3 = CreditCardFraudModelDense(\n",
    "    input_dim=X_train.shape[1], use_dropout=True, p=0.3\n",
    ")\n",
    "criterion_v1_3 = nn.BCEWithLogitsLoss(pos_weight=torch.sqrt(pos_weight))\n",
    "optimizer_v1_3 = torch.optim.Adam(model_v1_3.parameters(), lr=0.0001)\n",
    "\n",
    "result_v1_3 = train_and_evaluate_model(\n",
    "    name=\"MLP_v1.3_with_sqrt_pos_weights\",\n",
    "    model=model_v1_3,\n",
    "    criterion=criterion_v1_3,\n",
    "    optimizer=optimizer_v1_3,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v1_3[\"name\"]] = {\n",
    "    \"version\": 1.3,\n",
    "    \"val_aps\": result_v1_3[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v1_3[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v1_3[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v1_3[\"name\"]] = result_v1_3[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v1_3['name']}\")\n",
    "print(f\"Validation APS: {result_v1_3['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v1_3[\"train_losses\"],\n",
    "    result_v1_3[\"val_losses\"],\n",
    "    title=f\"{result_v1_3['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 3 Layers (wider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditCardFraudModelDenseWide(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, use_dropout: bool = False, p: float = 0.3\n",
    "    ) -> None:\n",
    "        super(CreditCardFraudModelDenseWide, self).__init__()\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(p=p))\n",
    "\n",
    "        layers.extend(\n",
    "            [\n",
    "                nn.Linear(32, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16, 1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return cast(torch.Tensor, self.network(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v2 without pos_weights,  1e-4 lr, dropout 0.3\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v2 = CreditCardFraudModelDenseWide(\n",
    "    input_dim=X_train.shape[1], use_dropout=True, p=0.3\n",
    ")\n",
    "criterion_v2 = nn.BCEWithLogitsLoss(pos_weight=None)\n",
    "optimizer_v2 = torch.optim.Adam(model_v2.parameters(), lr=0.0001)\n",
    "\n",
    "result_v2 = train_and_evaluate_model(\n",
    "    name=\"MLP_v2_without_pos_weights\",\n",
    "    model=model_v2,\n",
    "    criterion=criterion_v2,\n",
    "    optimizer=optimizer_v2,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v2[\"name\"]] = {\n",
    "    \"version\": 2,\n",
    "    \"val_aps\": result_v2[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v2[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v2[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v2[\"name\"]] = result_v2[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v2['name']}\")\n",
    "print(f\"Validation APS: {result_v2['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v2[\"train_losses\"],\n",
    "    result_v2[\"val_losses\"],\n",
    "    title=f\"{result_v2['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 3 Layers + aggressive dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v2.1 without pos_weights,  1e-4 lr, dropout 0.5\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v2_1 = CreditCardFraudModelDenseWide(\n",
    "    input_dim=X_train.shape[1], use_dropout=True, p=0.5\n",
    ")\n",
    "criterion_v2_1 = nn.BCEWithLogitsLoss(pos_weight=None)\n",
    "optimizer_v2_1 = torch.optim.Adam(model_v2_1.parameters(), lr=0.0001)\n",
    "\n",
    "result_v2_1 = train_and_evaluate_model(\n",
    "    name=\"MLP_v2.1_without_pos_weights\",\n",
    "    model=model_v2_1,\n",
    "    criterion=criterion_v2_1,\n",
    "    optimizer=optimizer_v2_1,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v2_1[\"name\"]] = {\n",
    "    \"version\": 2.1,\n",
    "    \"val_aps\": result_v2_1[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v2_1[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v2_1[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v2_1[\"name\"]] = result_v2_1[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v2_1['name']}\")\n",
    "print(f\"Validation APS: {result_v2_1['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v2_1[\"train_losses\"],\n",
    "    result_v2_1[\"val_losses\"],\n",
    "    title=f\"{result_v2_1['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### 3 layers + milder class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v2.2 with sqrt pos_weights,  1e-4 lr, dropout 0.3\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v2_2 = CreditCardFraudModelDenseWide(\n",
    "    input_dim=X_train.shape[1], use_dropout=True, p=0.3\n",
    ")\n",
    "criterion_v2_2 = nn.BCEWithLogitsLoss(pos_weight=pos_weight.sqrt())\n",
    "optimizer_v2_2 = torch.optim.Adam(model_v2_2.parameters(), lr=0.0001)\n",
    "\n",
    "result_v2_2 = train_and_evaluate_model(\n",
    "    name=\"MLP_v2.2_with_sqrt_pos_weights\",\n",
    "    model=model_v2_2,\n",
    "    criterion=criterion_v2_2,\n",
    "    optimizer=optimizer_v2_2,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v2_2[\"name\"]] = {\n",
    "    \"version\": 2.2,\n",
    "    \"val_aps\": result_v2_2[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v2_2[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v2_2[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v2_2[\"name\"]] = result_v2_2[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v2_2['name']}\")\n",
    "print(f\"Validation APS: {result_v2_2['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v2_2[\"train_losses\"],\n",
    "    result_v2_2[\"val_losses\"],\n",
    "    title=f\"{result_v2_2['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### 3 layers + milder class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP v2.3 with one-tenth sqrt pos_weights,  1e-4 lr, dropout 0.3\n",
    "set_seed(SEED)  # reset rng\n",
    "model_v2_3 = CreditCardFraudModelDenseWide(\n",
    "    input_dim=X_train.shape[1], use_dropout=True, p=0.3\n",
    ")\n",
    "criterion_v2_3 = nn.BCEWithLogitsLoss(pos_weight=pos_weight.sqrt() / 10)\n",
    "optimizer_v2_3 = torch.optim.Adam(model_v2_3.parameters(), lr=0.0001)\n",
    "\n",
    "result_v2_3 = train_and_evaluate_model(\n",
    "    name=\"MLP_v2.3_with_one_tenth_sqrt_pos_weights\",\n",
    "    model=model_v2_3,\n",
    "    criterion=criterion_v2_3,\n",
    "    optimizer=optimizer_v2_3,\n",
    "    train_loader=train_loader,\n",
    "    X_val_tensor=X_validation_tensor,\n",
    "    y_val_np=y_validation_np,\n",
    "    num_epochs=50,\n",
    ")\n",
    "\n",
    "experiments[result_v2_3[\"name\"]] = {\n",
    "    \"version\": 2.3,\n",
    "    \"val_aps\": result_v2_3[\"val_aps\"],\n",
    "    \"final_train_loss\": result_v2_3[\"train_losses\"][-1],\n",
    "    \"final_val_loss\": result_v2_3[\"val_losses\"][-1],\n",
    "}\n",
    "\n",
    "trained_models[result_v2_3[\"name\"]] = result_v2_3[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "print(f\"Experiment: {result_v2_3['name']}\")\n",
    "print(f\"Validation APS: {result_v2_3['val_aps']:.4f}\")\n",
    "plot_loss_curves(\n",
    "    result_v2_3[\"train_losses\"],\n",
    "    result_v2_3[\"val_losses\"],\n",
    "    title=f\"{result_v2_3['name']} Loss Curves\",\n",
    "    smoothing=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot validation APS for all experiments\n",
    "experiments_val_aps = pd.DataFrame(experiments).T.sort_values(\n",
    "    by=\"version\", ascending=True\n",
    ")[\"val_aps\"]\n",
    "plt.figure(figsize=(10, 4))\n",
    "ax = sns.barplot(x=experiments_val_aps.index, y=experiments_val_aps.values)\n",
    "ax.bar_label(\n",
    "    cast(BarContainer, ax.containers[0]),\n",
    "    fmt=\"%.4f\",\n",
    "    label_type=\"edge\",\n",
    "    padding=2,\n",
    "    fontsize=8,\n",
    ")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Average Precision Score\")\n",
    "plt.title(\"Deep Learning Model Comparison on Validation Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Evaluate best model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose MLP model\n",
    "set_seed(SEED)\n",
    "model_v1_2.eval()\n",
    "with torch.no_grad():\n",
    "    val_logits_v1_2 = model_v1_2(X_validation_tensor)\n",
    "    val_probs_v1_2 = torch.sigmoid(val_logits_v1_2).numpy().ravel()\n",
    "aps = average_precision_score(y_validation_np, val_probs_v1_2)\n",
    "print(f\"Validation APS for MLP_v1.2_without_pos_weights: {aps:.4f}\")\n",
    "\n",
    "# choose MLP model\n",
    "set_seed(SEED)\n",
    "model_v2_3.eval()\n",
    "with torch.no_grad():\n",
    "    val_logits_v2_3 = model_v2_3(X_validation_tensor)\n",
    "    val_probs_v2_3 = torch.sigmoid(val_logits_v2_3).numpy().ravel()\n",
    "aps = average_precision_score(y_validation_np, val_probs_v2_3)\n",
    "print(f\"Validation APS for MLP_v2.3_without_pos_weights: {aps:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Compare against ML baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ml baseline scores\n",
    "ml_baseline_scores = pd.read_csv(\"../results/ml_baselines/validation_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision-recall curves\n",
    "validation_precision_v14, validation_recall_v14, _ = precision_recall_curve(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"V14\"]\n",
    ")\n",
    "validation_precision_lr, validation_recall_lr, _ = precision_recall_curve(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"Logistic_Regression\"]\n",
    ")\n",
    "validation_precision_rf, validation_recall_rf, _ = precision_recall_curve(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"Random_Forest\"]\n",
    ")\n",
    "validation_precision_xgb, validation_recall_xgb, _ = precision_recall_curve(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"XGBoost\"]\n",
    ")\n",
    "validation_precision_model_v1_2, validation_recall_model_v1_2, _ = (\n",
    "    precision_recall_curve(y_validation_np, val_probs_v1_2)\n",
    ")\n",
    "# calculate average precision scores\n",
    "validation_aps_v14 = average_precision_score(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"V14\"]\n",
    ")\n",
    "validation_aps_lr = average_precision_score(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"Logistic_Regression\"]\n",
    ")\n",
    "validation_aps_rf = average_precision_score(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"Random_Forest\"]\n",
    ")\n",
    "validation_aps_xgb = average_precision_score(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"XGBoost\"]\n",
    ")\n",
    "validation_aps_model_v1_2 = average_precision_score(y_validation_np, val_probs_v1_2)\n",
    "\n",
    "# plot precision-recall curves\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# plot V14 VRC\n",
    "plt.plot(\n",
    "    validation_precision_v14,\n",
    "    validation_recall_v14,\n",
    "    lw=2,\n",
    "    label=f\"V14 | APS = {validation_aps_v14:.3f}\",\n",
    ")\n",
    "plt.fill_between(validation_precision_v14, validation_recall_v14, alpha=0.2)\n",
    "\n",
    "# plot LT PRC\n",
    "plt.plot(\n",
    "    validation_precision_lr,\n",
    "    validation_recall_lr,\n",
    "    lw=2,\n",
    "    label=f\"LT | APS = {validation_aps_lr:.3f}\",\n",
    ")\n",
    "plt.fill_between(validation_precision_lr, validation_recall_lr, alpha=0.2)\n",
    "\n",
    "# plot RF PRC\n",
    "plt.plot(\n",
    "    validation_precision_rf,\n",
    "    validation_recall_rf,\n",
    "    lw=2,\n",
    "    label=f\"RF | APS = {validation_aps_rf:.3f}\",\n",
    ")\n",
    "plt.fill_between(validation_precision_rf, validation_recall_rf, alpha=0.2)\n",
    "\n",
    "# plot XGB PRC\n",
    "plt.plot(\n",
    "    validation_precision_xgb,\n",
    "    validation_recall_xgb,\n",
    "    lw=2,\n",
    "    label=f\"XGB | APS = {validation_aps_xgb:.3f}\",\n",
    ")\n",
    "plt.fill_between(validation_precision_xgb, validation_recall_xgb, alpha=0.2)\n",
    "\n",
    "# plot MLP v1.2 PRC\n",
    "plt.plot(\n",
    "    validation_precision_model_v1_2,\n",
    "    validation_recall_model_v1_2,\n",
    "    lw=2,\n",
    "    label=f\"MLP v1.2 | APS = {validation_aps_model_v1_2:.3f}\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    validation_precision_model_v1_2, validation_recall_model_v1_2, alpha=0.2\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Recall (frauds caught)\")\n",
    "plt.ylabel(\"Precision (alerts correct)\")\n",
    "plt.title(\n",
    "    \"Precision-Recall Curve | Validation Set | V14 vs Classic ML Baselines vs Deep Learning\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot aps by model\n",
    "model_names = [\"V14\", \"Logistic Regression\", \"Random Forest\", \"XGBoost\", \"MLP v1.2\"]\n",
    "validation_aps_scores = [\n",
    "    validation_aps_v14,\n",
    "    validation_aps_lr,\n",
    "    validation_aps_rf,\n",
    "    validation_aps_xgb,\n",
    "    validation_aps_model_v1_2,\n",
    "]\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = sns.barplot(\n",
    "    x=model_names,\n",
    "    y=validation_aps_scores,\n",
    "    hue=model_names,\n",
    "    palette=[\"gray\", \"blue\", \"green\", \"orange\", \"red\"],\n",
    ")\n",
    "# add value labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(\n",
    "        cast(BarContainer, container),\n",
    "        fmt=\"%.4f\",\n",
    "        label_type=\"edge\",\n",
    "        padding=2,\n",
    "        fontsize=8,\n",
    "    )\n",
    "plt.ylabel(\"Average Precision Score (APS)\")\n",
    "plt.title(\"Model Comparison on Validation Set\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose MLP model\n",
    "set_seed(SEED)\n",
    "model_v1_2.eval()\n",
    "with torch.no_grad():\n",
    "    test_logits_v1_2 = model_v1_2(X_test_tensor)\n",
    "    test_probs_v1_2 = torch.sigmoid(test_logits_v1_2).numpy().ravel()\n",
    "aps = average_precision_score(y_test_np, test_probs_v1_2)\n",
    "print(f\"Test APS for MLP_v1.2_without_pos_weights: {aps:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ml baseline scores\n",
    "ml_baseline_scores = pd.read_csv(\"../results/ml_baselines/test_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision-recall curves\n",
    "test_precision_v14, test_recall_v14, _ = precision_recall_curve(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"V14\"]\n",
    ")\n",
    "test_precision_lr, test_recall_lr, _ = precision_recall_curve(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"Logistic_Regression\"]\n",
    ")\n",
    "test_precision_rf, test_recall_rf, _ = precision_recall_curve(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"Random_Forest\"]\n",
    ")\n",
    "test_precision_xgb, test_recall_xgb, _ = precision_recall_curve(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"XGBoost\"]\n",
    ")\n",
    "test_precision_model_v1_2, test_recall_model_v1_2, _ = precision_recall_curve(\n",
    "    y_test_np, test_probs_v1_2\n",
    ")\n",
    "# calculate average precision scores\n",
    "test_aps_v14 = average_precision_score(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"V14\"]\n",
    ")\n",
    "test_aps_lr = average_precision_score(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"Logistic_Regression\"]\n",
    ")\n",
    "test_aps_rf = average_precision_score(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"Random_Forest\"]\n",
    ")\n",
    "test_aps_xgb = average_precision_score(\n",
    "    ml_baseline_scores[\"y_true\"], ml_baseline_scores[\"XGBoost\"]\n",
    ")\n",
    "test_aps_model_v1_2 = average_precision_score(y_test_np, test_probs_v1_2)\n",
    "\n",
    "# plot precision-recall curves\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# plot V14 VRC\n",
    "plt.plot(\n",
    "    test_precision_v14, test_recall_v14, lw=2, label=f\"V14 | APS = {test_aps_v14:.3f}\"\n",
    ")\n",
    "plt.fill_between(test_precision_v14, test_recall_v14, alpha=0.2)\n",
    "\n",
    "# plot LT PRC\n",
    "plt.plot(test_precision_lr, test_recall_lr, lw=2, label=f\"LT | APS = {test_aps_lr:.3f}\")\n",
    "plt.fill_between(test_precision_lr, test_recall_lr, alpha=0.2)\n",
    "\n",
    "# plot RF PRC\n",
    "plt.plot(test_precision_rf, test_recall_rf, lw=2, label=f\"RF | APS = {test_aps_rf:.3f}\")\n",
    "plt.fill_between(test_precision_rf, test_recall_rf, alpha=0.2)\n",
    "\n",
    "# plot XGB PRC\n",
    "plt.plot(\n",
    "    test_precision_xgb, test_recall_xgb, lw=2, label=f\"XGB | APS = {test_aps_xgb:.3f}\"\n",
    ")\n",
    "plt.fill_between(test_precision_xgb, test_recall_xgb, alpha=0.2)\n",
    "\n",
    "# plot MLP v1.2 PRC\n",
    "plt.plot(\n",
    "    test_precision_model_v1_2,\n",
    "    test_recall_model_v1_2,\n",
    "    lw=2,\n",
    "    label=f\"MLP v1.2 | APS = {test_aps_model_v1_2:.3f}\",\n",
    ")\n",
    "plt.fill_between(test_precision_model_v1_2, test_recall_model_v1_2, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Recall (frauds caught)\")\n",
    "plt.ylabel(\"Precision (alerts correct)\")\n",
    "plt.title(\n",
    "    \"Precision-Recall Curve | Test Set | V14 vs Classic ML Baselines vs Deep Learning\"\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplot aps by model\n",
    "model_names = [\"V14\", \"Logistic Regression\", \"Random Forest\", \"XGBoost\", \"MLP v1.2\"]\n",
    "test_aps_scores = [\n",
    "    test_aps_v14,\n",
    "    test_aps_lr,\n",
    "    test_aps_rf,\n",
    "    test_aps_xgb,\n",
    "    test_aps_model_v1_2,\n",
    "]\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = sns.barplot(\n",
    "    x=model_names,\n",
    "    y=test_aps_scores,\n",
    "    hue=model_names,\n",
    "    palette=[\"gray\", \"blue\", \"green\", \"orange\", \"red\"],\n",
    ")\n",
    "# add value labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(\n",
    "        cast(BarContainer, container),\n",
    "        fmt=\"%.4f\",\n",
    "        label_type=\"edge\",\n",
    "        padding=2,\n",
    "        fontsize=8,\n",
    "    )\n",
    "plt.ylabel(\"Average Precision Score (APS)\")\n",
    "plt.title(\"Model Comparison on Test Set\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (credit-card-fraud-detection)",
   "language": "python",
   "name": "credit-card-fraud-detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
